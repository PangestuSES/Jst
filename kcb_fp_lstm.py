# -*- coding: utf-8 -*-
"""KCB_FP_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LH1wGV4QAktAnJ06_C8oc0k0_RYYVaY-
"""

# import library
import pandas as pd
import numpy as np
# cleaning data
from sklearn.preprocessing import LabelEncoder

# membaca data set dari file csv
from google.colab import drive
drive.mount('/content/drive')

path = "/content/drive/MyDrive/Colab Notebooks/seattle-weather.csv"
df = pd.read_csv(path)

df.head(10)

import matplotlib.pyplot as plt
import seaborn as sns

# Membuat chart untuk isi data
sns.countplot(data=df,x="weather")
plt.show();

# Melihat Persentase taget dari dataset
countrain=len(df[df.weather=="rain"])
countsun=len(df[df.weather=="sun"])
countdrizzle=len(df[df.weather=="drizzle"])
countsnow=len(df[df.weather=="snow"])
countfog=len(df[df.weather=="fog"])
print("Percent of Rain\t\t: {:.2f}%".format((countrain/(len(df.weather))*100)))
print("Percent of Sun\t\t: {:.2f}%".format((countsun/(len(df.weather))*100)))
print("Percent of Drizzle\t: {:.2f}%".format((countdrizzle/(len(df.weather))*100)))
print("Percent of Snow\t\t: {:.2f}%".format((countsnow/(len(df.weather))*100)))
print("Percent of Fog\t\t: {:.2f}%".format((countfog/(len(df.weather))*100)))

#Pembulatan Precipitation
pengendapan = []

for pt in df['precipitation']:
  #tambah nilai
  pengendapan.append(round(pt))

df['precipitation-round']=pengendapan

temp_cols = df.columns.tolist()
index = df.columns.get_loc("precipitation")
pengendapan_index = df.columns.get_loc("precipitation-round")


new_cols1 = (
    temp_cols[:index] +
    temp_cols[pengendapan_index:pengendapan_index+1] +
    temp_cols[index+1:pengendapan_index]
)

df = df[new_cols1]

df.head(5)

#Pembulatan temp_min
suhu_min = []

for s_min in df['temp_min']:
  #tambah nilai
  suhu_min.append(round(s_min))

df['temp_min-round']=suhu_min

temp_cols = df.columns.tolist()
index = df.columns.get_loc("temp_min")
suhu_min_index = df.columns.get_loc("temp_min-round")


new_cols2 = (
    temp_cols[:index] +
    temp_cols[suhu_min_index:suhu_min_index+1] +
    temp_cols[index+1:suhu_min_index]
)

df = df[new_cols2]

df.head(5)

#Pembulatan temp_max
suhu_max = []

for s_max in df['temp_max']:
  #tambah nilai
  suhu_max.append(round(s_max))

df['temp_max-round']=suhu_max

temp_cols = df.columns.tolist()
index = df.columns.get_loc("temp_max")
suhu_max_index = df.columns.get_loc("temp_max-round")


new_cols3 = (
    temp_cols[:index] +
    temp_cols[suhu_max_index:suhu_max_index+1] +
    temp_cols[index+1:suhu_max_index]
)

df = df[new_cols3]

df.head(5)

#Pembulatan wind
angin = []

for agn in df['wind']:
  #tambah nilai
  angin.append(round(agn))

df['wind-round']=angin

temp_cols = df.columns.tolist()
index = df.columns.get_loc("wind")
angin_index = df.columns.get_loc("wind-round")


new_cols4 = (
    temp_cols[:index] +
    temp_cols[angin_index:angin_index+1] +
    temp_cols[index+1:angin_index]
)

df = df[new_cols4]

df.head(5)

#Slicing Bulan contoh 2012-03-01 ->
bulan = []

for bl in df['date']:
  #tambah bulan
  bulan.append(int(bl[5:7]))  #"03" -> 3

df['date-bulan']=bulan

temp_cols = df.columns.tolist()
index = df.columns.get_loc("date")
bulan_index = df.columns.get_loc("date-bulan")


new_cols5 = (
    temp_cols[:index] +
    temp_cols[bulan_index:bulan_index+1] +
    temp_cols[index+1:bulan_index]
)

df = df[new_cols5]

df.head(5)

#encoder
enc = LabelEncoder()

# weather [0 -> Drizzle, 1 -> fog, 2-> rain, 3 -> snow, 4 -> sun]
df.weather = enc.fit_transform(df.weather.values)
display(df.iloc[192])

#data
df.head(200)

#Mendapatkan ukuran csv

size_data = len(df)

#Membagi untuk data latih (80%)
train = round(size_data*0.8)
train_set = df[:train]


#membagi untuk data uji (20%)
test_set = df[train:]

# Alokasikan sampel data latih
x_train = train_set[[
    'date-bulan',
    'precipitation-round',
    'temp_max-round',
    'temp_min-round',
    'wind-round',
    ]].values
y_train = train_set.weather.values



type(x_train) # numpy.ndarray

# Model architecture Neural Network

import tensorflow as tf
from tensorflow import keras
from keras import Sequential
from keras.layers import Dense
from keras.layers import LSTM

model = Sequential([
    LSTM(units = 25, input_shape= (5,1), activation = 'elu'),
    Dense(units = 50, activation = 'elu'),
    Dense(units = 5, activation = 'softmax')
])

"""## Fungsi aktivasi Elu menggabungkan karakteristik dari fungsi ReLU dan fungsi sigmoid. Fungsi aktivasi Elu mengatasi masalah ini dengan mengizinkan nilai negatif pada input neuron.

## Fungsi aktivasi Softmax adalah fungsi yang umum digunakan dalam jaringan saraf tiruan untuk masalah klasifikasi multikelas. Fungsi ini mengubah keluaran dari lapisan terakhir jaringan menjadi distribusi probabilitas yang menjumlahkan menjadi 1, dengan setiap elemen probabilitas yang merepresentasikan peluang kelas tertentu.
"""

model.summary()

from keras.optimizers import Adamax

model.compile(loss = 'sparse_categorical_crossentropy', optimizer= Adamax(learning_rate = 0.0001),
              metrics = ['accuracy']
             )

"""## Adamax: Variasi dari optimizer Adam yang lebih stabil dalam kasus jaringan dengan parameter yang besar."""

# training the model
LSTM_history = model.fit(x = x_train,
                         y = y_train,
                         batch_size= 10,
                         epochs = 100,
                         validation_split= 0.1,
                         shuffle = True,
                         verbose = 2)

# Grafik Perbandingan Training dan Validation
acc = LSTM_history.history['accuracy']
val_acc = LSTM_history.history['val_accuracy']

loss = LSTM_history.history['loss']
val_loss = LSTM_history.history['val_loss']

plt.figure(figsize=(12, 12))

plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy', color='r')
plt.plot(val_acc, label='Validation Accuracy', color='b')
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend(loc='lower right', fontsize=13)
plt.ylabel('Accuracy', fontsize=16, weight='bold')
plt.title('LSTM - Training & Validation Acc.', fontsize=16, weight='bold')
plt.xlabel('Epoch', fontsize=15, weight='bold')
plt.subplots_adjust(hspace=0.5)
plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss', color='r')
plt.plot(val_loss, label='Validation Loss', color='b')
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.legend(loc='upper right', fontsize=13)
plt.ylabel('Cross Entropy', fontsize=16, weight='bold')
plt.title('LSTM - Training & Validation Loss', fontsize=15, weight='bold')
plt.xlabel('Epoch', fontsize=15, weight='bold')

plt.show()

# alokasi sampel data uji
x_test = test_set[[
    'date-bulan',
    'precipitation-round',
    'temp_max-round',
    'temp_min-round',
    'wind-round',
    ]].values
y_test = test_set.weather.values


type(x_test) # numpy.ndarray
print(x_test)

# Model Prediction
predictions = model.predict(x = x_test,
                            batch_size= 10,
                            verbose=0)
print(predictions)

# Model Evaluate
accuracy = model.evaluate(x_test, y_test)

# ringkasan dari metrik evaluasi untuk tugas klasifikasi
from sklearn.metrics import classification_report

y_pred = np.argmax(predictions, axis=1)

print(classification_report(y_test, y_pred))

# Menghitung nilai akurasi dengan index prediksi dengan kemungkinan tertinggi
rounded_pred = np.argmax(predictions, axis = 1)

count = 0
for i in range(len(rounded_pred)):
  row = test_set.iloc[i]
  if rounded_pred[i] == row['weather']:
    count += 1

print("%.2f" %(count/len(rounded_pred)*100), "% akurasi")